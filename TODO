TODO:


1. Chord pretraining.
  - is this an experiment on its own?

Some other time:
1 multiscale RNN. Quarter notes. Complete bars?
2 Bidirectional generator.
* CNN discriminator?

1. nan/inf.
   What needs to be figured out?
     - does minibatch features for mean_d help? learn useful stuff?
     - they seem to mostly result in ones. WHY?
     - perhaps they assumed sigmoidal features. trying to apply logistic.
       + applying sigmoid on the output from the tensor multiplication helps. But what does it mean?

* Similarities/dissimilarities with original image paper?
* Similarities/dissimilarities with ML training of LSTMs?
* Normalize output from G?

* CNN generator? How?
* MLP discriminator?

DONE:
1. Do some plotting.
  - Polyphony: how often are two tones played simultaneously?
  - Interval distributions. Report the top-k intervals, except 0?
* read at random pos in midi file.
* Feature matching
* Scoring output.
* Bidirectional discriminator
* Look at real music in internal representation.
* Perhaps make cells correspond to a number of ticks, and not arbitrary number of microsecs?
* (32th) And then, make ticks less frequent. Perhaps max resolution a sixteenth note?
* If big difference in loss: let the worst loss be optimized. Literature calls this freezing. I invented it before looking it up.
* Pretrain as normal RNN?
* Remove all zero-units. Instead put in breaks with a length attribute. new cli argument: variable_ticks
What works?
 - lower learningrate
 - minibatch features for mean_d.
 - mean d with one decision per data point.
 - mean over each decision for each tone
 - lower learning rate helps with infs/nans
What doesn't?
 - bidirectional with minibatch features
   * does minibatch layer learn anything useful?
   * inf means no gradient, i guess

